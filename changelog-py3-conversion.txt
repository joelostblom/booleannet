# This file lists all the changes made by the 2to3 automatic conversion and the manual edits done afterwards in order to make all the tests run without errors in Python3.

commit 19661421232f7c1c5a374fff51e6f61a0a50b909
Author: joelostblom <joel.ostblom@gmail.com>
Date:   Thu Jul 7 09:00:36 2016 -0400

    2to3 and manual python 3 updates

diff --git a/.gitignore b/.gitignore
new file mode 100644
index 0000000..6e8722f
--- /dev/null
+++ b/.gitignore
@@ -0,0 +1,2 @@
+./build
+./dist
diff --git a/boolean2/__init__.py b/boolean2/__init__.py
index 7b18479..44e0d84 100644
--- a/boolean2/__init__.py
+++ b/boolean2/__init__.py
@@ -6,15 +6,15 @@ import sys, re, os
 
 __VERSION__ = '1.2.0-beta'
 
-import util
+from . import util
 
 # require python 2.4 or higher
 if sys.version_info[:2] < (2, 5):
     util.error("this program requires python 2.5 or higher" )
 
-import ruleparser, boolmodel, timemodel, tokenizer
+from . import ruleparser, boolmodel, timemodel, tokenizer
 
-from tokenizer import modify_states
+from .tokenizer import modify_states
 
 def Model( text, mode):
     "Factory function that returns the proper class based on the mode"
@@ -33,7 +33,7 @@ def Model( text, mode):
     elif mode == ruleparser.PLDE:
         # matplotlib may not be installed 
         # so defer import to allow other modes to be used
-        from plde import model
+        from .plde import model
         return model.PldeModel( mode='plde', text=text)
     else:
         return boolmodel.BoolModel( mode=mode, text=text )
@@ -58,7 +58,7 @@ def test():
     model.initialize(  )
     model.iterate( steps=10, fullt=2)
     
-    print all_nodes ( text )
+    print(all_nodes ( text ))
     #for i in range(12):
     #    print model.next()
 
diff --git a/boolean2/boolmodel.py b/boolean2/boolmodel.py
index 6378481..b5a704b 100644
--- a/boolean2/boolmodel.py
+++ b/boolean2/boolmodel.py
@@ -1,6 +1,6 @@
-import util
-from ruleparser import Parser
-import tokenizer, util, state
+from . import util
+from .ruleparser import Parser
+from . import tokenizer, util, state
 
 class BoolModel(Parser):
     """
@@ -23,7 +23,7 @@ class BoolModel(Parser):
         self.states = self.parser.states = [ self.parser.old ]
 
         # parser the initial data
-        map( self.local_parse, self.init_lines )
+        list(map( self.local_parse, self.init_lines ))
 
         # deal with uninitialized nodes
         if self.uninit_nodes:
@@ -37,7 +37,7 @@ class BoolModel(Parser):
                 util.error( 'uninitialized nodes: %s' % list(self.uninit_nodes))
 
         # override any initalization with defaults
-        for node, value in defaults.items():
+        for node, value in list(defaults.items()):
             self.parser.RULE_SETVALUE( self.parser.old, node, value, None)
             self.parser.RULE_SETVALUE( self.parser.new, node, value, None)
         
@@ -63,7 +63,7 @@ class BoolModel(Parser):
         # this is an expensive operation so it loads lazily
         assert self.states, 'States are empty'
         if not self.lazy_data:
-            nodes = self.first.keys()
+            nodes = list(self.first.keys())
             for state in self.states:
                 for node in nodes:
                     self.lazy_data.setdefault( node, []).append( state[node] )
@@ -90,13 +90,13 @@ class BoolModel(Parser):
         # needs to be reset in case the data changes
         self.lazy_data = {}
 
-        for index in xrange(steps):
+        for index in range(steps):
             self.parser.RULE_START_ITERATION( index, self )
             self.state_update()
             for rank in self.ranks:
                 lines = self.update_lines[rank]
                 lines = shuffler( lines )
-                map( self.local_parse, lines ) 
+                list(map( self.local_parse, lines )) 
 
     def save_states(self, fname):
         """
@@ -104,11 +104,11 @@ class BoolModel(Parser):
         """
         if self.states:
             fp = open(fname, 'wt')
-            cols = [ 'STATE' ] + self.first.keys() 
+            cols = [ 'STATE' ] + list(self.first.keys()) 
             hdrs = util.join ( cols )
             fp.write( hdrs )
             for state in self.states:
-                cols = [ state.fp() ] + state.values()
+                cols = [ state.fp() ] + list(state.values())
                 line = util.join( cols )
                 fp.write( line )
             fp.close()
@@ -126,11 +126,11 @@ class BoolModel(Parser):
         index, size = self.detect_cycles()
         
         if size == 0:
-            print "No cycle or steady state could be detected from the %d states" % len(self.states)
+            print("No cycle or steady state could be detected from the %d states" % len(self.states))
         elif size==1:
-            print "Steady state starting at index %s -> %s" % (index, self.states[index] )
+            print("Steady state starting at index %s -> %s" % (index, self.states[index] ))
         else:
-            print "Cycle of length %s starting at index %s" % (size, index)
+            print("Cycle of length %s starting at index %s" % (size, index))
     
     def fp(self):
         "The models current fingerprint"
@@ -154,18 +154,18 @@ if __name__ == '__main__':
 
     model.initialize(  )
         
-    print '>>>', model.first
+    print('>>>', model.first)
 
     model.iterate( steps=2 )
     
-    print model.fp()
+    print(model.fp())
     model.report_cycles()
     model.save_states( fname='states.txt' )
 
     # detect cycles from a list of states
     states = ['S1', 'S2', 'S1', 'S2', 'S1', 'S2']
-    print 
-    print 'States %s -> Detect cycles %s' % (states, util.detect_cycles( states ) )
+    print() 
+    print('States %s -> Detect cycles %s' % (states, util.detect_cycles( states ) ))
 
 
        
\ No newline at end of file
diff --git a/boolean2/network.py b/boolean2/network.py
index 38771af..0aa39aa 100644
--- a/boolean2/network.py
+++ b/boolean2/network.py
@@ -1,4 +1,4 @@
-import util
+from . import util
 import random
 from itertools import count
 
@@ -79,7 +79,7 @@ class TransGraph(object):
         "Adds states to the transition"
     
         # generating the fingerprints and sto
-        times = times or range(len(states))
+        times = times or list(range(len(states)))
         fprints = []
         for state in states:
             if self.verbose:
@@ -105,19 +105,19 @@ class TransGraph(object):
         self.fp.write( '*** node values ***\n' )
 
         # writes the mapping
-        first = self.store.values()[0]
-        header = [ 'state' ] + first.keys()
+        first = list(self.store.values())[0]
+        header = [ 'state' ] + list(first.keys())
         self.fp.write( util.join(header) )
         
         for fprint, state in sorted( self.store.items() ):
-            line = [ fprint ]  + map(int, state.values() )
+            line = [ fprint ]  + list(map(int, list(state.values()) ))
             self.fp.write( util.join(line) )
 
 def test():
     """
     Main testrunnner
     """
-    import boolmodel
+    from . import boolmodel
     
     text = """
     A = True
diff --git a/boolean2/odict.py b/boolean2/odict.py
index 7955ff9..64cdf83 100644
--- a/boolean2/odict.py
+++ b/boolean2/odict.py
@@ -35,7 +35,7 @@ class odict(DictMixin):
     def __init__(self, **kwds):
         self._keys = []
         self._data = {}
-        for key, value in kwds.items():
+        for key, value in list(kwds.items()):
             self[key] = value
         
     def __setitem__(self, key, value):
diff --git a/boolean2/plde/helper.py b/boolean2/plde/helper.py
index 2fd4a12..e96d9f5 100644
--- a/boolean2/plde/helper.py
+++ b/boolean2/plde/helper.py
@@ -1,7 +1,7 @@
 """
 Helper functions
 """
-import csv, StringIO
+import csv, io
 import string
 from itertools import *
 
@@ -14,7 +14,7 @@ except:
     pass
 """
 
-from defs import *
+from .defs import *
 
 def change(node, indexer):
     "Returns the change for a node"
@@ -58,7 +58,7 @@ def hill_func( node, indexer, par):
     index = indexer[node]
     try:
         text = ' hill( c%d, h=%s, n=%s ) ' % ( index, par[node].h, par[node].n )
-    except Exception, exc:
+    except Exception as exc:
         msg = "error creating hill function for node %s -> %s" % (node, exc)
         raise Exception(msg)
     return text
@@ -71,7 +71,7 @@ def prop_func( node, indexer, par):
     try:
         nconc = conc(node, indexer)
         text = ' prop( r=%s, rc=%s ) - %s ' % ( par[node].r, par[node].rc, nconc )
-    except Exception, exc:
+    except Exception as exc:
         msg = "error creating proportion function for node %s -> %s" % (node, exc)
         raise Exception(msg)
     return text
@@ -132,7 +132,7 @@ def initializer(data, labels=None, **kwds):
         try:
             values = [ data[node][label] for label in labels ]
             return tuple(values) 
-        except KeyError, exc:
+        except KeyError as exc:
             
             if 'default' in kwds:
                 return kwds['default']
@@ -189,10 +189,10 @@ def read_parameters( fname ):
     
     def something( row ):
         # skips rows with empty elements
-        return filter(lambda x:x, map(string.strip, row ))
+        return [x for x in map(string.strip, row ) if x]
     
     # load the file, skipping commented or empty rows
-    lines = filter( something, csv.reader( CommentedFile(fname))) 
+    lines = list(filter( something, csv.reader( CommentedFile(fname)))) 
 
     # check file size 
     assert len(lines) > 2, "file '%s' needs to have more than two lines" % fname
@@ -205,7 +205,7 @@ def read_parameters( fname ):
             raise Exception( "column number mismatch expected %d, found %s, at line '%s'" % (size, colnum, ', '.join(elems)))
         return True
     
-    lines = filter( coltest, lines )
+    lines = list(filter( coltest, lines ))
     
     # nodes and attributes
     nodes, attrs = lines[0:2]
@@ -213,7 +213,7 @@ def read_parameters( fname ):
     # tries to coerce the value into a datastructure, float tuples, float or string
     def tuple_cast( word ):
         try:
-            values = map( float, word.split(',') )
+            values = list(map( float, word.split(',') ))
             if len( values ) > 1 :
                 return tuple(values)
             else:
@@ -241,10 +241,10 @@ class CommentedFile:
             fp = file(fp, 'rU')
         self.fp = fp
 
-    def next(self):
-        line = self.fp.next()
+    def __next__(self):
+        line = next(self.fp)
         while line.startswith('#'):
-            line = self.fp.next()
+            line = next(self.fp)
         return line
 
     def __iter__(self):
diff --git a/boolean2/plde/model.py b/boolean2/plde/model.py
index 623d601..18b1d3b 100644
--- a/boolean2/plde/model.py
+++ b/boolean2/plde/model.py
@@ -4,7 +4,8 @@ from itertools import *
 
 from boolean2.boolmodel import BoolModel
 from boolean2 import util, odict, tokenizer
-import helper
+from . import helper
+import imp
 
 try:
     import pylab
@@ -80,7 +81,7 @@ class PldeModel( BoolModel ):
             self.indexer[node] = index
         
         # a sanity check
-        assert self.nodes == self.mapper.keys()
+        assert self.nodes == list(self.mapper.keys())
 
     def generate_init( self, localdefs ):
         """
@@ -91,9 +92,9 @@ class PldeModel( BoolModel ):
         init.extend( self.EXTRA_INIT.splitlines() )
         init.append( '# dynamically generated code' )
         init.append( '# abbreviations: c=concentration, d=decay, t=threshold, n=newvalue' )
-        init.append( '# %s' % self.mapper.values() )
+        init.append( '# %s' % list(self.mapper.values()) )
 
-        for index, node, triplet in self.mapper.values():
+        for index, node, triplet in list(self.mapper.values()):
             conc, decay, tresh = boolmapper(triplet)
             #assert decay > 0, 'Decay for node %s must be larger than 0 -> %s' % (node, str(triplet))   
             store = dict( index=index, conc=conc, decay=decay, tresh=tresh, node=node)
@@ -130,10 +131,10 @@ class PldeModel( BoolModel ):
         """
         sep = ' ' * 4
 
-        indices = [ x[0] for x in self.mapper.values() ]
+        indices = [ x[0] for x in list(self.mapper.values()) ]
         assign  = [ 'c%d' % i for i in indices ]
         retvals = [ 'n%d' % i for i in indices ]
-        zeros   = map(lambda x: '0.0', indices ) 
+        zeros   = ['0.0' for x in indices] 
         assign  = ', '.join(assign)
         retvals = ', '.join(retvals)
         zeros   = ', '.join( zeros )
@@ -189,8 +190,8 @@ class PldeModel( BoolModel ):
                 os.remove( '%s.pyc' % autogen )
             except OSError:
                 pass # must be a read only filesystem
-            reload( autogen_mod )
-        except Exception, exc:
+            imp.reload( autogen_mod )
+        except Exception as exc:
             msg = "'%s' in:\n%s\n*** dynamic code error ***\n%s" % ( exc, self.dynamic_code, exc )
             util.error(msg)
 
diff --git a/boolean2/ply/lex.py b/boolean2/ply/lex.py
index 71a2679..e33fc5c 100644
--- a/boolean2/ply/lex.py
+++ b/boolean2/ply/lex.py
@@ -25,6 +25,7 @@
 __version__ = "2.3"
 
 import re, sys, types
+import collections
 
 # Regular expression used to match valid token names
 _is_identifier = re.compile(r'^[a-zA-Z0-9_]+$')
@@ -34,7 +35,7 @@ _is_identifier = re.compile(r'^[a-zA-Z0-9_]+$')
 # with Python 2.0 where types.ObjectType is undefined.
 
 try:
-   _INSTANCETYPE = (types.InstanceType, types.ObjectType)
+   _INSTANCETYPE = type(object) #(types.InstanceType, object)
 except AttributeError:
    _INSTANCETYPE = types.InstanceType
    class object: pass       # Note: needed if no new-style classes present
@@ -117,7 +118,7 @@ class Lexer:
 
         if object:
             newtab = { }
-            for key, ritem in self.lexstatere.items():
+            for key, ritem in list(self.lexstatere.items()):
                 newre = []
                 for cre, findex in ritem:
                      newfindex = []
@@ -130,7 +131,7 @@ class Lexer:
                 newtab[key] = newre
             c.lexstatere = newtab
             c.lexstateerrorf = { }
-            for key, ef in self.lexstateerrorf.items():
+            for key, ef in list(self.lexstateerrorf.items()):
                 c.lexstateerrorf[key] = getattr(object,ef.__name__)
             c.lexmodule = object
 
@@ -150,7 +151,7 @@ class Lexer:
         tf.write("_lexstateinfo = %s\n" % repr(self.lexstateinfo))
         
         tabre = { }
-        for key, lre in self.lexstatere.items():
+        for key, lre in list(self.lexstatere.items()):
              titem = []
              for i in range(len(lre)):
                   titem.append((self.lexstateretext[key][i],_funcs_to_names(lre[i][1])))
@@ -160,7 +161,7 @@ class Lexer:
         tf.write("_lexstateignore = %s\n" % repr(self.lexstateignore))
 
         taberr = { }
-        for key, ef in self.lexstateerrorf.items():
+        for key, ef in list(self.lexstateerrorf.items()):
              if ef:
                   taberr[key] = ef.__name__
              else:
@@ -172,7 +173,7 @@ class Lexer:
     # readtab() - Read lexer information from a tab file
     # ------------------------------------------------------------
     def readtab(self,tabfile,fdict):
-        exec "import %s as lextab" % tabfile
+        exec("import %s as lextab" % tabfile)
         self.lextokens      = lextab._lextokens
         self.lexreflags     = lextab._lexreflags
         self.lexliterals    = lextab._lexliterals
@@ -180,7 +181,7 @@ class Lexer:
         self.lexstateignore = lextab._lexstateignore
         self.lexstatere     = { }
         self.lexstateretext = { }
-        for key,lre in lextab._lexstatere.items():
+        for key,lre in list(lextab._lexstatere.items()):
              titem = []
              txtitem = []
              for i in range(len(lre)):
@@ -189,7 +190,7 @@ class Lexer:
              self.lexstatere[key] = titem
              self.lexstateretext[key] = txtitem
         self.lexstateerrorf = { }
-        for key,ef in lextab._lexstateerrorf.items():
+        for key,ef in list(lextab._lexstateerrorf.items()):
              self.lexstateerrorf[key] = fdict[ef]
         self.begin('INITIAL')
          
@@ -197,8 +198,8 @@ class Lexer:
     # input() - Push a new string into the lexer
     # ------------------------------------------------------------
     def input(self,s):
-        if not (isinstance(s,types.StringType) or isinstance(s,types.UnicodeType)):
-            raise ValueError, "Expected a string"
+        if not (isinstance(s,bytes) or isinstance(s,str)):
+            raise ValueError("Expected a string")
         self.lexdata = s
         self.lexpos = 0
         self.lexlen = len(s)
@@ -207,8 +208,8 @@ class Lexer:
     # begin() - Changes the lexing state
     # ------------------------------------------------------------
     def begin(self,state):
-        if not self.lexstatere.has_key(state):
-            raise ValueError, "Undefined state"
+        if state not in self.lexstatere:
+            raise ValueError("Undefined state")
         self.lexre = self.lexstatere[state]
         self.lexretext = self.lexstateretext[state]
         self.lexignore = self.lexstateignore.get(state,"")
@@ -286,7 +287,7 @@ class Lexer:
                    break
 
                 # if func not callable, it means it's an ignored token                
-                if not callable(func):
+                if not isinstance(func, collections.Callable):
                    break 
 
                 # If token is processed by a function, call it
@@ -299,9 +300,9 @@ class Lexer:
                 
                 # Verify type of the token.  If not in the token map, raise an error
                 if not self.lexoptimize:
-                    if not self.lextokens.has_key(newtok.type):
-                        raise LexError, ("%s:%d: Rule '%s' returned an unknown token type '%s'" % (
-                            func.func_code.co_filename, func.func_code.co_firstlineno,
+                    if newtok.type not in self.lextokens:
+                        raise LexError("%s:%d: Rule '%s' returned an unknown token type '%s'" % (
+                            func.__code__.co_filename, func.__code__.co_firstlineno,
                             func.__name__, newtok.type),lexdata[lexpos:])
 
                 return newtok
@@ -329,17 +330,17 @@ class Lexer:
                     newtok = self.lexerrorf(tok)
                     if lexpos == self.lexpos:
                         # Error method didn't change text position at all. This is an error.
-                        raise LexError, ("Scanning error. Illegal character '%s'" % (lexdata[lexpos]), lexdata[lexpos:])
+                        raise LexError("Scanning error. Illegal character '%s'" % (lexdata[lexpos]), lexdata[lexpos:])
                     lexpos = self.lexpos
                     if not newtok: continue
                     return newtok
 
                 self.lexpos = lexpos
-                raise LexError, ("Illegal character '%s' at index %d" % (lexdata[lexpos],lexpos), lexdata[lexpos:])
+                raise LexError("Illegal character '%s' at index %d" % (lexdata[lexpos],lexpos), lexdata[lexpos:])
 
         self.lexpos = lexpos + 1
         if self.lexdata is None:
-             raise RuntimeError, "No input string given with input()"
+             raise RuntimeError("No input string given with input()")
         return None
         
 # -----------------------------------------------------------------------------
@@ -377,7 +378,7 @@ def _validate_file(filename):
             if not prev:
                 counthash[name] = linen
             else:
-                print >>sys.stderr, "%s:%d: Rule %s redefined. Previously defined on line %d" % (filename,linen,name,prev)
+                print("%s:%d: Rule %s redefined. Previously defined on line %d" % (filename,linen,name,prev), file=sys.stderr)
                 noerror = 0
         linen += 1
     return noerror
@@ -430,7 +431,7 @@ def _form_master_re(relist,reflags,ldict,toknames):
 
         # Build the index to function map for the matching engine
         lexindexfunc = [ None ] * (max(lexre.groupindex.values())+1)
-        for f,i in lexre.groupindex.items():
+        for f,i in list(lexre.groupindex.items()):
             handle = ldict.get(f,None)
             if type(handle) in (types.FunctionType, types.MethodType):
                 lexindexfunc[i] = (handle,toknames[handle.__name__])
@@ -443,7 +444,7 @@ def _form_master_re(relist,reflags,ldict,toknames):
                     lexindexfunc[i] = (None, toknames[f])
          
         return [(lexre,lexindexfunc)],[regex]
-    except Exception,e:
+    except Exception as e:
         m = int(len(relist)/2)
         if m == 0: m = 1
         llist, lre = _form_master_re(relist[:m],reflags,ldict,toknames)
@@ -463,7 +464,7 @@ def _statetoken(s,names):
     nonstate = 1
     parts = s.split("_")
     for i in range(1,len(parts)):
-         if not names.has_key(parts[i]) and parts[i] != 'ANY': break
+         if parts[i] not in names and parts[i] != 'ANY': break
     if i > 1:
        states = tuple(parts[1:i])
     else:
@@ -506,7 +507,7 @@ def lex(module=None,object=None,debug=0,optimize=0,lextab="lextab",reflags=0,now
             for (i,v) in _items:
                 ldict[i] = v
         else:
-            raise ValueError,"Expected a module or instance"
+            raise ValueError("Expected a module or instance")
         lexobj.lexmodule = module
         
     else:
@@ -541,67 +542,67 @@ def lex(module=None,object=None,debug=0,optimize=0,lextab="lextab",reflags=0,now
         literals = ldict.get("literals","")
         
     if not tokens:
-        raise SyntaxError,"lex: module does not define 'tokens'"
-    if not (isinstance(tokens,types.ListType) or isinstance(tokens,types.TupleType)):
-        raise SyntaxError,"lex: tokens must be a list or tuple."
+        raise SyntaxError("lex: module does not define 'tokens'")
+    if not (isinstance(tokens,list) or isinstance(tokens,tuple)):
+        raise SyntaxError("lex: tokens must be a list or tuple.")
 
     # Build a dictionary of valid token names
     lexobj.lextokens = { }
     if not optimize:
         for n in tokens:
             if not _is_identifier.match(n):
-                print >>sys.stderr, "lex: Bad token name '%s'" % n
+                print("lex: Bad token name '%s'" % n, file=sys.stderr)
                 error = 1
-            if warn and lexobj.lextokens.has_key(n):
-                print >>sys.stderr, "lex: Warning. Token '%s' multiply defined." % n
+            if warn and n in lexobj.lextokens:
+                print("lex: Warning. Token '%s' multiply defined." % n, file=sys.stderr)
             lexobj.lextokens[n] = None
     else:
         for n in tokens: lexobj.lextokens[n] = None
 
     if debug:
-        print "lex: tokens = '%s'" % lexobj.lextokens.keys()
+        print("lex: tokens = '%s'" % list(lexobj.lextokens.keys()))
 
     try:
          for c in literals:
-               if not (isinstance(c,types.StringType) or isinstance(c,types.UnicodeType)) or len(c) > 1:
-                    print >>sys.stderr, "lex: Invalid literal %s. Must be a single character" % repr(c)
+               if not (isinstance(c,bytes) or isinstance(c,str)) or len(c) > 1:
+                    print("lex: Invalid literal %s. Must be a single character" % repr(c), file=sys.stderr)
                     error = 1
                     continue
 
     except TypeError:
-         print >>sys.stderr, "lex: Invalid literals specification. literals must be a sequence of characters."
+         print("lex: Invalid literals specification. literals must be a sequence of characters.", file=sys.stderr)
          error = 1
 
     lexobj.lexliterals = literals
 
     # Build statemap
     if states:
-         if not (isinstance(states,types.TupleType) or isinstance(states,types.ListType)):
-              print >>sys.stderr, "lex: states must be defined as a tuple or list."
+         if not (isinstance(states,tuple) or isinstance(states,list)):
+              print("lex: states must be defined as a tuple or list.", file=sys.stderr)
               error = 1
          else:
               for s in states:
-                    if not isinstance(s,types.TupleType) or len(s) != 2:
-                           print >>sys.stderr, "lex: invalid state specifier %s. Must be a tuple (statename,'exclusive|inclusive')" % repr(s)
+                    if not isinstance(s,tuple) or len(s) != 2:
+                           print("lex: invalid state specifier %s. Must be a tuple (statename,'exclusive|inclusive')" % repr(s), file=sys.stderr)
                            error = 1
                            continue
                     name, statetype = s
-                    if not isinstance(name,types.StringType):
-                           print >>sys.stderr, "lex: state name %s must be a string" % repr(name)
+                    if not isinstance(name,bytes):
+                           print("lex: state name %s must be a string" % repr(name), file=sys.stderr)
                            error = 1
                            continue
                     if not (statetype == 'inclusive' or statetype == 'exclusive'):
-                           print >>sys.stderr, "lex: state type for state %s must be 'inclusive' or 'exclusive'" % name
+                           print("lex: state type for state %s must be 'inclusive' or 'exclusive'" % name, file=sys.stderr)
                            error = 1
                            continue
-                    if stateinfo.has_key(name):
-                           print >>sys.stderr, "lex: state '%s' already defined." % name
+                    if name in stateinfo:
+                           print("lex: state '%s' already defined." % name, file=sys.stderr)
                            error = 1
                            continue
                     stateinfo[name] = statetype
 
     # Get a list of symbols with the t_ or s_ prefix
-    tsymbols = [f for f in ldict.keys() if f[:2] == 't_' ]
+    tsymbols = [f for f in list(ldict.keys()) if f[:2] == 't_' ]
 
     # Now build up a list of functions and a list of strings
 
@@ -609,7 +610,7 @@ def lex(module=None,object=None,debug=0,optimize=0,lextab="lextab",reflags=0,now
     strsym =   { }        # Symbols defined as strings
     toknames = { }        # Mapping of symbols to token names
 
-    for s in stateinfo.keys():
+    for s in list(stateinfo.keys()):
          funcsym[s] = []
          strsym[s] = []
 
@@ -617,62 +618,62 @@ def lex(module=None,object=None,debug=0,optimize=0,lextab="lextab",reflags=0,now
     errorf   = { }        # Error functions by state
 
     if len(tsymbols) == 0:
-        raise SyntaxError,"lex: no rules of the form t_rulename are defined."
+        raise SyntaxError("lex: no rules of the form t_rulename are defined.")
 
     for f in tsymbols:
         t = ldict[f]
         states, tokname = _statetoken(f,stateinfo)
         toknames[f] = tokname
 
-        if callable(t):
+        if isinstance(t, collections.Callable):
             for s in states: funcsym[s].append((f,t))
-        elif (isinstance(t, types.StringType) or isinstance(t,types.UnicodeType)):
+        elif (isinstance(t, bytes) or isinstance(t,str)):
             for s in states: strsym[s].append((f,t))
         else:
-            print >>sys.stderr, "lex: %s not defined as a function or string" % f
+            print("lex: %s not defined as a function or string" % f, file=sys.stderr)
             error = 1
 
     # Sort the functions by line number
-    for f in funcsym.values():
-        f.sort(lambda x,y: cmp(x[1].func_code.co_firstlineno,y[1].func_code.co_firstlineno))
+    for f in list(funcsym.values()):
+        f.sort(lambda x,y: cmp(x[1].__code__.co_firstlineno,y[1].__code__.co_firstlineno))
 
     # Sort the strings by regular expression length
-    for s in strsym.values():
+    for s in list(strsym.values()):
         s.sort(lambda x,y: (len(x[1]) < len(y[1])) - (len(x[1]) > len(y[1])))
 
     regexs = { }
 
     # Build the master regular expressions
-    for state in stateinfo.keys():
+    for state in list(stateinfo.keys()):
         regex_list = []
 
         # Add rules defined by functions first
         for fname, f in funcsym[state]:
-            line = f.func_code.co_firstlineno
-            file = f.func_code.co_filename
+            line = f.__code__.co_firstlineno
+            file = f.__code__.co_filename
             files[file] = None
             tokname = toknames[fname]
 
             ismethod = isinstance(f, types.MethodType)
 
             if not optimize:
-                nargs = f.func_code.co_argcount
+                nargs = f.__code__.co_argcount
                 if ismethod:
                     reqargs = 2
                 else:
                     reqargs = 1
                 if nargs > reqargs:
-                    print >>sys.stderr, "%s:%d: Rule '%s' has too many arguments." % (file,line,f.__name__)
+                    print("%s:%d: Rule '%s' has too many arguments." % (file,line,f.__name__), file=sys.stderr)
                     error = 1
                     continue
 
                 if nargs < reqargs:
-                    print >>sys.stderr, "%s:%d: Rule '%s' requires an argument." % (file,line,f.__name__)
+                    print("%s:%d: Rule '%s' requires an argument." % (file,line,f.__name__), file=sys.stderr)
                     error = 1
                     continue
 
                 if tokname == 'ignore':
-                    print >>sys.stderr, "%s:%d: Rule '%s' must be defined as a string." % (file,line,f.__name__)
+                    print("%s:%d: Rule '%s' must be defined as a string." % (file,line,f.__name__), file=sys.stderr)
                     error = 1
                     continue
         
@@ -685,25 +686,25 @@ def lex(module=None,object=None,debug=0,optimize=0,lextab="lextab",reflags=0,now
                     try:
                         c = re.compile("(?P<%s>%s)" % (f.__name__,f.__doc__), re.VERBOSE | reflags)
                         if c.match(""):
-                             print >>sys.stderr, "%s:%d: Regular expression for rule '%s' matches empty string." % (file,line,f.__name__)
+                             print("%s:%d: Regular expression for rule '%s' matches empty string." % (file,line,f.__name__), file=sys.stderr)
                              error = 1
                              continue
-                    except re.error,e:
-                        print >>sys.stderr, "%s:%d: Invalid regular expression for rule '%s'. %s" % (file,line,f.__name__,e)
+                    except re.error as e:
+                        print("%s:%d: Invalid regular expression for rule '%s'. %s" % (file,line,f.__name__,e), file=sys.stderr)
                         if '#' in f.__doc__:
-                             print >>sys.stderr, "%s:%d. Make sure '#' in rule '%s' is escaped with '\\#'." % (file,line, f.__name__)                 
+                             print("%s:%d. Make sure '#' in rule '%s' is escaped with '\\#'." % (file,line, f.__name__), file=sys.stderr)                 
                         error = 1
                         continue
 
                     if debug:
-                        print "lex: Adding rule %s -> '%s' (state '%s')" % (f.__name__,f.__doc__, state)
+                        print("lex: Adding rule %s -> '%s' (state '%s')" % (f.__name__,f.__doc__, state))
 
                 # Okay. The regular expression seemed okay.  Let's append it to the master regular
                 # expression we're building
   
                 regex_list.append("(?P<%s>%s)" % (f.__name__,f.__doc__))
             else:
-                print >>sys.stderr, "%s:%d: No regular expression defined for rule '%s'" % (file,line,f.__name__)
+                print("%s:%d: No regular expression defined for rule '%s'" % (file,line,f.__name__), file=sys.stderr)
 
         # Now add all of the simple rules
         for name,r in strsym[state]:
@@ -711,68 +712,68 @@ def lex(module=None,object=None,debug=0,optimize=0,lextab="lextab",reflags=0,now
 
             if tokname == 'ignore':
                  if "\\" in r:
-                      print >>sys.stderr, "lex: Warning. %s contains a literal backslash '\\'" % name
+                      print("lex: Warning. %s contains a literal backslash '\\'" % name, file=sys.stderr)
                  ignore[state] = r
                  continue
 
             if not optimize:
                 if tokname == 'error':
-                    raise SyntaxError,"lex: Rule '%s' must be defined as a function" % name
+                    raise SyntaxError("lex: Rule '%s' must be defined as a function" % name)
                     error = 1
                     continue
         
-                if not lexobj.lextokens.has_key(tokname) and tokname.find("ignore_") < 0:
-                    print >>sys.stderr, "lex: Rule '%s' defined for an unspecified token %s." % (name,tokname)
+                if tokname not in lexobj.lextokens and tokname.find("ignore_") < 0:
+                    print("lex: Rule '%s' defined for an unspecified token %s." % (name,tokname), file=sys.stderr)
                     error = 1
                     continue
                 try:
                     c = re.compile("(?P<%s>%s)" % (name,r),re.VERBOSE | reflags)
                     if (c.match("")):
-                         print >>sys.stderr, "lex: Regular expression for rule '%s' matches empty string." % name
+                         print("lex: Regular expression for rule '%s' matches empty string." % name, file=sys.stderr)
                          error = 1
                          continue
-                except re.error,e:
-                    print >>sys.stderr, "lex: Invalid regular expression for rule '%s'. %s" % (name,e)
+                except re.error as e:
+                    print("lex: Invalid regular expression for rule '%s'. %s" % (name,e), file=sys.stderr)
                     if '#' in r:
-                         print >>sys.stderr, "lex: Make sure '#' in rule '%s' is escaped with '\\#'." % name
+                         print("lex: Make sure '#' in rule '%s' is escaped with '\\#'." % name, file=sys.stderr)
 
                     error = 1
                     continue
                 if debug:
-                    print "lex: Adding rule %s -> '%s' (state '%s')" % (name,r,state)
+                    print("lex: Adding rule %s -> '%s' (state '%s')" % (name,r,state))
                 
             regex_list.append("(?P<%s>%s)" % (name,r))
 
         if not regex_list:
-             print >>sys.stderr, "lex: No rules defined for state '%s'" % state
+             print("lex: No rules defined for state '%s'" % state, file=sys.stderr)
              error = 1
 
         regexs[state] = regex_list
 
 
     if not optimize:
-        for f in files.keys(): 
+        for f in list(files.keys()): 
            if not _validate_file(f):
                 error = 1
 
     if error:
-        raise SyntaxError,"lex: Unable to build lexer."
+        raise SyntaxError("lex: Unable to build lexer.")
 
     # From this point forward, we're reasonably confident that we can build the lexer.
     # No more errors will be generated, but there might be some warning messages.
 
     # Build the master regular expressions
 
-    for state in regexs.keys():
+    for state in list(regexs.keys()):
         lexre, re_text = _form_master_re(regexs[state],reflags,ldict,toknames)
         lexobj.lexstatere[state] = lexre
         lexobj.lexstateretext[state] = re_text
         if debug:
             for i in range(len(re_text)):
-                 print "lex: state '%s'. regex[%d] = '%s'" % (state, i, re_text[i])
+                 print("lex: state '%s'. regex[%d] = '%s'" % (state, i, re_text[i]))
 
     # For inclusive states, we need to add the INITIAL state
-    for state,type in stateinfo.items():
+    for state,type in list(stateinfo.items()):
         if state != "INITIAL" and type == 'inclusive':
              lexobj.lexstatere[state].extend(lexobj.lexstatere['INITIAL'])
              lexobj.lexstateretext[state].extend(lexobj.lexstateretext['INITIAL'])
@@ -789,19 +790,19 @@ def lex(module=None,object=None,debug=0,optimize=0,lextab="lextab",reflags=0,now
     lexobj.lexstateerrorf = errorf
     lexobj.lexerrorf = errorf.get("INITIAL",None)
     if warn and not lexobj.lexerrorf:
-        print >>sys.stderr, "lex: Warning. no t_error rule is defined."
+        print("lex: Warning. no t_error rule is defined.", file=sys.stderr)
 
     # Check state information for ignore and error rules
-    for s,stype in stateinfo.items():
+    for s,stype in list(stateinfo.items()):
         if stype == 'exclusive':
-              if warn and not errorf.has_key(s):
-                   print >>sys.stderr, "lex: Warning. no error rule is defined for exclusive state '%s'" % s
-              if warn and not ignore.has_key(s) and lexobj.lexignore:
-                   print >>sys.stderr, "lex: Warning. no ignore rule is defined for exclusive state '%s'" % s
+              if warn and s not in errorf:
+                   print("lex: Warning. no error rule is defined for exclusive state '%s'" % s, file=sys.stderr)
+              if warn and s not in ignore and lexobj.lexignore:
+                   print("lex: Warning. no ignore rule is defined for exclusive state '%s'" % s, file=sys.stderr)
         elif stype == 'inclusive':
-              if not errorf.has_key(s):
+              if s not in errorf:
                    errorf[s] = errorf.get("INITIAL",None)
-              if not ignore.has_key(s):
+              if s not in ignore:
                    ignore[s] = ignore.get("INITIAL","")
    
 
@@ -830,7 +831,7 @@ def runmain(lexer=None,data=None):
             data = f.read()
             f.close()
         except IndexError:
-            print "Reading from standard input (type EOF to end):"
+            print("Reading from standard input (type EOF to end):")
             data = sys.stdin.read()
 
     if lexer:
@@ -846,7 +847,7 @@ def runmain(lexer=None,data=None):
     while 1:
         tok = _token()
         if not tok: break
-        print "(%s,%r,%d,%d)" % (tok.type, tok.value, tok.lineno,tok.lexpos)
+        print("(%s,%r,%d,%d)" % (tok.type, tok.value, tok.lineno,tok.lexpos))
         
 
 # -----------------------------------------------------------------------------
diff --git a/boolean2/ply/yacc.py b/boolean2/ply/yacc.py
index 2993f0b..c37ce5a 100644
--- a/boolean2/ply/yacc.py
+++ b/boolean2/ply/yacc.py
@@ -67,7 +67,7 @@ default_lr  = 'LALR'           # Default LR table generation method
 
 error_count = 3                # Number of symbols that must be shifted to leave recovery mode
 
-import re, types, sys, cStringIO, md5, os.path
+import re, types, sys, io, hashlib, os.path, functools
 
 # Exception raised for yacc-related errors
 class YaccError(Exception):   pass
@@ -77,7 +77,8 @@ class YaccError(Exception):   pass
 # with Python 2.0 where types.ObjectType is undefined.
 
 try:
-   _INSTANCETYPE = (types.InstanceType, types.ObjectType)
+   #_INSTANCETYPE = (types.InstanceType, object)
+   _INSTANCETYPE = type(object)
 except AttributeError:
    _INSTANCETYPE = types.InstanceType
    class object: pass     # Note: needed if no new-style classes present
@@ -148,9 +149,9 @@ class YaccProduction:
 
     def pushback(self,n):
         if n <= 0:
-            raise ValueError, "Expected a positive value"
+            raise ValueError("Expected a positive value")
         if n > (len(self.slice)-1):
-            raise ValueError, "Can't push %d tokens. Only %d are available." % (n,len(self.slice)-1)
+            raise ValueError("Can't push %d tokens. Only %d are available." % (n,len(self.slice)-1))
         for i in range(0,n):
             self.pbstack.append(self.slice[-i-1])
 
@@ -166,7 +167,7 @@ class Parser:
         # object directly.
 
         if magic != "xyzzy":
-            raise YaccError, "Can't instantiate Parser. Use yacc() instead."
+            raise YaccError("Can't instantiate Parser. Use yacc() instead.")
 
         # Reset internal state
         self.productions = None          # List of productions
@@ -198,7 +199,7 @@ class Parser:
 
         # If no lexer was given, we will try to use the lex module
         if not lexer:
-            import lex
+            import ply.lex as lex
             lexer = lex.lexer
         
         pslice.lexer = lexer
@@ -231,7 +232,7 @@ class Parser:
             # is already set, we just use that. Otherwise, we'll pull
             # the next token off of the lookaheadstack or from the lexer
             if debug > 1:
-                print 'state', state
+                print('state', state)
             if not lookahead:
                 if not lookaheadstack:
                     lookahead = get_token()     # Get the next token
@@ -248,7 +249,7 @@ class Parser:
             t = actions[state].get(ltype)
 
             if debug > 1:
-                print 'action', t
+                print('action', t)
             if t is not None:
                 if t > 0:
                     # shift a symbol on the stack
@@ -408,7 +409,7 @@ class Parser:
                 continue
 
             # Call an error function here
-            raise RuntimeError, "yacc: internal parser error!!!\n"
+            raise RuntimeError("yacc: internal parser error!!!\n")
 
 # -----------------------------------------------------------------------------
 #                          === Parser Construction ===
@@ -463,18 +464,18 @@ def validate_file(filename):
 
 # This function looks for functions that might be grammar rules, but which don't have the proper p_suffix.
 def validate_dict(d):
-    for n,v in d.items(): 
+    for n,v in list(d.items()): 
         if n[0:2] == 'p_' and type(v) in (types.FunctionType, types.MethodType): continue
         if n[0:2] == 't_': continue
 
         if n[0:2] == 'p_':
             sys.stderr.write("yacc: Warning. '%s' not defined as a function\n" % n)
-        if 1 and isinstance(v,types.FunctionType) and v.func_code.co_argcount == 1:
+        if 1 and isinstance(v,types.FunctionType) and v.__code__.co_argcount == 1:
             try:
                 doc = v.__doc__.split(" ")
                 if doc[1] == ':':
-                    sys.stderr.write("%s:%d: Warning. Possible grammar rule '%s' defined without p_ prefix.\n" % (v.func_code.co_filename, v.func_code.co_firstlineno,n))
-            except StandardError:
+                    sys.stderr.write("%s:%d: Warning. Possible grammar rule '%s' defined without p_ prefix.\n" % (v.__code__.co_filename, v.__code__.co_firstlineno,n))
+            except Exception:
                 pass
 
 # -----------------------------------------------------------------------------
@@ -518,7 +519,7 @@ def initialize_vars():
 
     Errorfunc    = None    # User defined error handler
 
-    Signature    = md5.new()   # Digital signature of the grammar rules, precedence
+    Signature    = hashlib.new('md5')   # Digital signature of the grammar rules, precedence
                                # and other information.  Used to determined when a
                                # parsing table needs to be regenerated.
 
@@ -526,8 +527,8 @@ def initialize_vars():
 
     # File objects used when creating the parser.out debugging file
     global _vf, _vfc
-    _vf           = cStringIO.StringIO()
-    _vfc          = cStringIO.StringIO()
+    _vf           = io.StringIO()
+    _vfc          = io.StringIO()
 
 # -----------------------------------------------------------------------------
 # class Production:
@@ -555,7 +556,7 @@ def initialize_vars():
 
 class Production:
     def __init__(self,**kw):
-        for k,v in kw.items():
+        for k,v in list(kw.items()):
             setattr(self,k,v)
         self.lr_index = -1
         self.lr0_added = 0    # Flag indicating whether or not added to LR0 closure
@@ -593,7 +594,7 @@ class Production:
         # Precompute list of productions immediately following
         try:
             p.lrafter = Prodnames[p.prod[n+1]]
-        except (IndexError,KeyError),e:
+        except (IndexError,KeyError) as e:
             p.lrafter = []
         try:
             p.lrbefore = p.prod[n-1]
@@ -627,7 +628,7 @@ _is_identifier = re.compile(r'^[a-zA-Z0-9_-]+$')
 
 def add_production(f,file,line,prodname,syms):
     
-    if Terminals.has_key(prodname):
+    if prodname in Terminals:
         sys.stderr.write("%s:%d: Illegal rule name '%s'. Already defined as a token.\n" % (file,line,prodname))
         return -1
     if prodname == 'error':
@@ -646,7 +647,7 @@ def add_production(f,file,line,prodname,syms):
                  if (len(c) > 1):
                       sys.stderr.write("%s:%d: Literal token %s in rule '%s' may only be a single character\n" % (file,line,s, prodname)) 
                       return -1
-                 if not Terminals.has_key(c):
+                 if c not in Terminals:
                       Terminals[c] = []
                  syms[x] = c
                  continue
@@ -658,7 +659,7 @@ def add_production(f,file,line,prodname,syms):
 
     # See if the rule is already in the rulemap
     map = "%s -> %s" % (prodname,syms)
-    if Prodmap.has_key(map):
+    if map in Prodmap:
         m = Prodmap[map]
         sys.stderr.write("%s:%d: Duplicate rule %s.\n" % (file,line, m))
         sys.stderr.write("%s:%d: Previous definition at %s:%d\n" % (file,line, m.file, m.line))
@@ -675,7 +676,7 @@ def add_production(f,file,line,prodname,syms):
             
     Productions.append(p)
     Prodmap[map] = p
-    if not Nonterminals.has_key(prodname):
+    if prodname not in Nonterminals:
         Nonterminals[prodname] = [ ]
     
     # Add all terminals to Terminals
@@ -699,13 +700,13 @@ def add_production(f,file,line,prodname,syms):
             del p.prod[i]
             continue
 
-        if Terminals.has_key(t):
+        if t in Terminals:
             Terminals[t].append(p.number)
             # Is a terminal.  We'll assign a precedence to p based on this
             if not hasattr(p,"prec"):
                 p.prec = Precedence.get(t,('right',0))
         else:
-            if not Nonterminals.has_key(t):
+            if t not in Nonterminals:
                 Nonterminals[t] = [ ]
             Nonterminals[t].append(p.number)
         i += 1
@@ -734,8 +735,8 @@ def add_production(f,file,line,prodname,syms):
 # and adds rules to the grammar
 
 def add_function(f):
-    line = f.func_code.co_firstlineno
-    file = f.func_code.co_filename
+    line = f.__code__.co_firstlineno
+    file = f.__code__.co_filename
     error = 0
 
     if isinstance(f,types.MethodType):
@@ -743,11 +744,11 @@ def add_function(f):
     else:
         reqdargs = 1
         
-    if f.func_code.co_argcount > reqdargs:
+    if f.__code__.co_argcount > reqdargs:
         sys.stderr.write("%s:%d: Rule '%s' has too many arguments.\n" % (file,line,f.__name__))
         return -1
 
-    if f.func_code.co_argcount < reqdargs:
+    if f.__code__.co_argcount < reqdargs:
         sys.stderr.write("%s:%d: Rule '%s' requires an argument.\n" % (file,line,f.__name__))
         return -1
           
@@ -788,7 +789,7 @@ def add_function(f):
                 error += e
 
                 
-            except StandardError:
+            except Exception:
                 sys.stderr.write("%s:%d: Syntax error in rule '%s'\n" % (file,dline,ps))
                 error -= 1
     else:
@@ -805,12 +806,12 @@ def compute_reachable():
     (Unused terminals have already had their warning.)
     '''
     Reachable = { }
-    for s in Terminals.keys() + Nonterminals.keys():
+    for s in list(Terminals.keys()) + list(Nonterminals.keys()):
         Reachable[s] = 0
 
     mark_reachable_from( Productions[0].prod[0], Reachable )
 
-    for s in Nonterminals.keys():
+    for s in list(Nonterminals.keys()):
         if not Reachable[s]:
             sys.stderr.write("yacc: Symbol '%s' is unreachable.\n" % s)
 
@@ -840,7 +841,7 @@ def compute_terminates():
     Terminates = {}
 
     # Terminals:
-    for t in Terminals.keys():
+    for t in list(Terminals.keys()):
         Terminates[t] = 1
 
     Terminates['$end'] = 1
@@ -848,13 +849,13 @@ def compute_terminates():
     # Nonterminals:
 
     # Initialize to false:
-    for n in Nonterminals.keys():
+    for n in list(Nonterminals.keys()):
         Terminates[n] = 0
 
     # Then propagate termination until no change:
     while 1:
         some_change = 0
-        for (n,pl) in Prodnames.items():
+        for (n,pl) in list(Prodnames.items()):
             # Nonterminal n terminates iff any of its productions terminates.
             for p in pl:
                 # Production p terminates iff all of its rhs symbols terminate.
@@ -882,9 +883,9 @@ def compute_terminates():
             break
 
     some_error = 0
-    for (s,terminates) in Terminates.items():
+    for (s,terminates) in list(Terminates.items()):
         if not terminates:
-            if not Prodnames.has_key(s) and not Terminals.has_key(s) and s != 'error':
+            if s not in Prodnames and s not in Terminals and s != 'error':
                 # s is used-but-not-defined, and we've already warned of that,
                 # so it would be overkill to say that it's also non-terminating.
                 pass
@@ -905,7 +906,7 @@ def verify_productions(cycle_check=1):
         if not p: continue
 
         for s in p.prod:
-            if not Prodnames.has_key(s) and not Terminals.has_key(s) and s != 'error':
+            if s not in Prodnames and s not in Terminals and s != 'error':
                 sys.stderr.write("%s:%d: Symbol '%s' used, but not defined as a token or a rule.\n" % (p.file,p.line,s))
                 error = 1
                 continue
@@ -914,7 +915,7 @@ def verify_productions(cycle_check=1):
     # Now verify all of the tokens
     if yaccdebug:
         _vf.write("Unused terminals:\n\n")
-    for s,v in Terminals.items():
+    for s,v in list(Terminals.items()):
         if s != 'error' and not v:
             sys.stderr.write("yacc: Warning. Token '%s' defined, but not used.\n" % s)
             if yaccdebug: _vf.write("   %s\n"% s)
@@ -928,7 +929,7 @@ def verify_productions(cycle_check=1):
         
     unused_prod = 0
     # Verify the use of all productions
-    for s,v in Nonterminals.items():
+    for s,v in list(Nonterminals.items()):
         if not v:
             p = Prodnames[s][0]
             sys.stderr.write("%s:%d: Warning. Rule '%s' defined, but not used.\n" % (p.file,p.line, s))
@@ -947,12 +948,12 @@ def verify_productions(cycle_check=1):
 
     if yaccdebug:
         _vf.write("\nTerminals, with rules where they appear\n\n")
-        ks = Terminals.keys()
+        ks = list(Terminals.keys())
         ks.sort()
         for k in ks:
             _vf.write("%-20s : %s\n" % (k, " ".join([str(s) for s in Terminals[k]])))
         _vf.write("\nNonterminals, with rules where they appear\n\n")
-        ks = Nonterminals.keys()
+        ks = list(Nonterminals.keys())
         ks.sort()
         for k in ks:
             _vf.write("%-20s : %s\n" % (k, " ".join([str(s) for s in Nonterminals[k]])))
@@ -1015,7 +1016,7 @@ def add_precedence(plist):
                 sys.stderr.write("yacc: Invalid precedence '%s'\n" % prec)
                 return -1
             for t in terms:
-                if Precedence.has_key(t):
+                if t in Precedence:
                     sys.stderr.write("yacc: Precedence already specified for terminal '%s'\n" % t)
                     error += 1
                     continue
@@ -1085,7 +1086,7 @@ def first(beta):
 
 def compute_follow(start=None):
     # Add '$end' to the follow list of the start symbol
-    for k in Nonterminals.keys():
+    for k in list(Nonterminals.keys()):
         Follow[k] = [ ]
 
     if not start:
@@ -1099,7 +1100,7 @@ def compute_follow(start=None):
             # Here is the production set
             for i in range(len(p.prod)):
                 B = p.prod[i]
-                if Nonterminals.has_key(B):
+                if B in Nonterminals:
                     # Okay. We got a non-terminal in a production
                     fst = first(p.prod[i+1:])
                     hasempty = 0
@@ -1119,7 +1120,7 @@ def compute_follow(start=None):
 
     if 0 and yaccdebug:
         _vf.write('\nFollow:\n')
-        for k in Nonterminals.keys():
+        for k in list(Nonterminals.keys()):
             _vf.write("%-20s : %s\n" % (k, " ".join([str(s) for s in Follow[k]])))
 
 # -------------------------------------------------------------------------
@@ -1130,7 +1131,7 @@ def compute_follow(start=None):
 def compute_first1():
 
     # Terminals:
-    for t in Terminals.keys():
+    for t in list(Terminals.keys()):
         First[t] = [t]
 
     First['$end'] = ['$end']
@@ -1139,13 +1140,13 @@ def compute_first1():
     # Nonterminals:
 
     # Initialize to the empty set:
-    for n in Nonterminals.keys():
+    for n in list(Nonterminals.keys()):
         First[n] = []
 
     # Then propagate symbols until no change:
     while 1:
         some_change = 0
-        for n in Nonterminals.keys():
+        for n in list(Nonterminals.keys()):
             for p in Prodnames[n]:
                 for f in first(p.prod):
                     if f not in First[n]:
@@ -1156,7 +1157,7 @@ def compute_first1():
 
     if 0 and yaccdebug:
         _vf.write('\nFirst:\n')
-        for k in Nonterminals.keys():
+        for k in list(Nonterminals.keys()):
             _vf.write("%-20s : %s\n" %
                 (k, " ".join([str(s) for s in First[k]])))
 
@@ -1268,10 +1269,10 @@ def lr0_items():
             for s in ii.usyms:
                 asyms[s] = None
 
-        for x in asyms.keys():
+        for x in list(asyms.keys()):
             g = lr0_goto(I,x)
             if not g:  continue
-            if _lr0_cidhash.has_key(id(g)): continue
+            if id(g) in _lr0_cidhash: continue
             _lr0_cidhash[id(g)] = len(C)            
             C.append(g)
             
@@ -1317,7 +1318,7 @@ def compute_nullable_nonterminals():
                 nullable[p.name] = 1
                 continue
            for t in p.prod:
-                if not nullable.has_key(t): break
+                if t not in nullable: break
            else:
                 nullable[p.name] = 1
        if len(nullable) == num_nullable: break
@@ -1341,7 +1342,7 @@ def find_nonterminal_transitions(C):
          for p in C[state]:
              if p.lr_index < p.len - 1:
                   t = (state,p.prod[p.lr_index+1])
-                  if Nonterminals.has_key(t[1]):
+                  if t[1] in Nonterminals:
                         if t not in trans: trans.append(t)
          state = state + 1
      return trans
@@ -1364,7 +1365,7 @@ def dr_relation(C,trans,nullable):
     for p in g:
        if p.lr_index < p.len - 1:
            a = p.prod[p.lr_index+1]
-           if Terminals.has_key(a):
+           if a in Terminals:
                if a not in terms: terms.append(a)
 
     # This extra bit is to handle the start state
@@ -1389,7 +1390,7 @@ def reads_relation(C, trans, empty):
     for p in g:
         if p.lr_index < p.len - 1:
              a = p.prod[p.lr_index + 1]
-             if empty.has_key(a):
+             if a in empty:
                   rel.append((j,a))
 
     return rel
@@ -1449,15 +1450,15 @@ def compute_lookback_includes(C,trans,nullable):
                  t = p.prod[lr_index]
 
                  # Check to see if this symbol and state are a non-terminal transition
-                 if dtrans.has_key((j,t)):
+                 if (j,t) in dtrans:
                        # Yes.  Okay, there is some chance that this is an includes relation
                        # the only way to know for certain is whether the rest of the 
                        # production derives empty
 
                        li = lr_index + 1
                        while li < p.len:
-                            if Terminals.has_key(p.prod[li]): break      # No forget it
-                            if not nullable.has_key(p.prod[li]): break
+                            if p.prod[li] in Terminals: break      # No forget it
+                            if p.prod[li] not in nullable: break
                             li = li + 1
                        else:
                             # Appears to be a relation between (j,t) and (state,N)
@@ -1478,7 +1479,7 @@ def compute_lookback_includes(C,trans,nullable):
                  else:
                       lookb.append((j,r))
         for i in includes:
-             if not includedict.has_key(i): includedict[i] = []
+             if i not in includedict: includedict[i] = []
              includedict[i].append((state,N))
         lookdict[(state,N)] = lookb
 
@@ -1525,11 +1526,11 @@ def traverse(x,N,stack,F,X,R,FP):
         for a in F.get(y,[]):
             if a not in F[x]: F[x].append(a)
     if N[x] == d:
-       N[stack[-1]] = sys.maxint
+       N[stack[-1]] = sys.maxsize
        F[stack[-1]] = F[x]
        element = stack.pop()
        while element != x:
-           N[stack[-1]] = sys.maxint
+           N[stack[-1]] = sys.maxsize
            F[stack[-1]] = F[x]
            element = stack.pop()
 
@@ -1586,10 +1587,10 @@ def compute_follow_sets(ntrans,readsets,inclsets):
 # -----------------------------------------------------------------------------
 
 def add_lookaheads(lookbacks,followset):
-    for trans,lb in lookbacks.items():
+    for trans,lb in list(lookbacks.items()):
         # Loop over productions in lookback
         for state,p in lb:
-             if not p.lookaheads.has_key(state):
+             if state not in p.lookaheads:
                   p.lookaheads[state] = []
              f = followset.get(trans,[])
              for a in f:
@@ -1724,7 +1725,7 @@ def lr_parse_table(method):
                 else:
                     i = p.lr_index
                     a = p.prod[i+1]       # Get symbol right after the "."
-                    if Terminals.has_key(a):
+                    if a in Terminals:
                         g = lr0_goto(I,a)
                         j = _lr0_cidhash.get(id(g),-1)
                         if j >= 0:
@@ -1766,23 +1767,23 @@ def lr_parse_table(method):
                                 st_action[a] = j
                                 st_actionp[a] = p
                                 
-            except StandardError,e:
-               print sys.exc_info()
-               raise YaccError, "Hosed in lr_parse_table"
+            except Exception as e:
+               print(sys.exc_info())
+               raise YaccError("Hosed in lr_parse_table")
 
         # Print the actions associated with each terminal
         if yaccdebug:
           _actprint = { }
           for a,p,m in actlist:
-            if st_action.has_key(a):
+            if a in st_action:
                 if p is st_actionp[a]:
                     _vf.write("    %-15s %s\n" % (a,m))
                     _actprint[(a,m)] = 1
           _vf.write("\n")
           for a,p,m in actlist:
-            if st_action.has_key(a):
+            if a in st_action:
                 if p is not st_actionp[a]:
-                    if not _actprint.has_key((a,m)):
+                    if (a,m) not in _actprint:
                         _vf.write("  ! %-15s [ %s ]\n" % (a,m))
                         _actprint[(a,m)] = 1
             
@@ -1792,9 +1793,9 @@ def lr_parse_table(method):
         nkeys = { }
         for ii in I:
             for s in ii.usyms:
-                if Nonterminals.has_key(s):
+                if s in Nonterminals:
                     nkeys[s] = None
-        for n in nkeys.keys():
+        for n in list(nkeys.keys()):
             g = lr0_goto(I,n)
             j = _lr0_cidhash.get(id(g),-1)            
             if j >= 0:
@@ -1849,8 +1850,8 @@ _lr_signature = %s
         if smaller:
             items = { }
 
-            for s,nd in _lr_action.items():
-               for name,v in nd.items():
+            for s,nd in list(_lr_action.items()):
+               for name,v in list(nd.items()):
                   i = items.get(name)
                   if not i:
                      i = ([],[])
@@ -1859,7 +1860,7 @@ _lr_signature = %s
                   i[1].append(v)
 
             f.write("\n_lr_action_items = {")
-            for k,v in items.items():
+            for k,v in list(items.items()):
                 f.write("%r:([" % k)
                 for i in v[0]:
                     f.write("%r," % i)
@@ -1881,7 +1882,7 @@ del _lr_action_items
             
         else:
             f.write("\n_lr_action = { ");
-            for k,v in _lr_action.items():
+            for k,v in list(_lr_action.items()):
                 f.write("(%r,%r):%r," % (k[0],k[1],v))
             f.write("}\n");
 
@@ -1889,8 +1890,8 @@ del _lr_action_items
             # Factor out names to try and make smaller
             items = { }
 
-            for s,nd in _lr_goto.items():
-               for name,v in nd.items():
+            for s,nd in list(_lr_goto.items()):
+               for name,v in list(nd.items()):
                   i = items.get(name)
                   if not i:
                      i = ([],[])
@@ -1899,7 +1900,7 @@ del _lr_action_items
                   i[1].append(v)
 
             f.write("\n_lr_goto_items = {")
-            for k,v in items.items():
+            for k,v in list(items.items()):
                 f.write("%r:([" % k)
                 for i in v[0]:
                     f.write("%r," % i)
@@ -1920,7 +1921,7 @@ del _lr_goto_items
 """)
         else:
             f.write("\n_lr_goto = { ");
-            for k,v in _lr_goto.items():
+            for k,v in list(_lr_goto.items()):
                 f.write("(%r,%r):%r," % (k[0],k[1],v))                    
             f.write("}\n");
 
@@ -1938,15 +1939,15 @@ del _lr_goto_items
         
         f.close()
 
-    except IOError,e:
-        print >>sys.stderr, "Unable to create '%s'" % filename
-        print >>sys.stderr, e
+    except IOError as e:
+        print("Unable to create '%s'" % filename, file=sys.stderr)
+        print(e, file=sys.stderr)
         return
 
 def lr_read_tables(module=tab_module,optimize=0):
     global _lr_action, _lr_goto, _lr_productions, _lr_method
     try:
-        exec "import %s as parsetab" % module
+        exec("import %s as parsetab" % module)
         
         if (optimize) or (Signature.digest() == parsetab._lr_signature):
             _lr_action = parsetab._lr_action
@@ -1967,7 +1968,7 @@ def lr_read_tables(module=tab_module,optimize=0):
 # Build the parser module
 # -----------------------------------------------------------------------------
 
-def yacc(method=default_lr, debug=yaccdebug, module=None, tabmodule=tab_module, start=None, check_recursion=1, optimize=0,write_tables=1,debugfile=debug_file,outputdir=''):
+def yacc(method=default_lr.encode('utf-8'), debug=yaccdebug, module=None, tabmodule=tab_module, start=None, check_recursion=1, optimize=0,write_tables=1,debugfile=debug_file,outputdir=''):
     global yaccdebug
     yaccdebug = debug
     
@@ -1992,7 +1993,7 @@ def yacc(method=default_lr, debug=yaccdebug, module=None, tabmodule=tab_module,
             for i in _items:
                 ldict[i[0]] = i[1]
         else:
-            raise ValueError,"Expected a module"
+            raise ValueError("Expected a module")
         
     else:
         # No module given.  We might be able to get information from the caller.
@@ -2038,24 +2039,24 @@ def yacc(method=default_lr, debug=yaccdebug, module=None, tabmodule=tab_module,
             tokens = ldict.get("tokens",None)
     
         if not tokens:
-            raise YaccError,"module does not define a list 'tokens'"
-        if not (isinstance(tokens,types.ListType) or isinstance(tokens,types.TupleType)):
-            raise YaccError,"tokens must be a list or tuple."
+            raise YaccError("module does not define a list 'tokens'")
+        if not (isinstance(tokens,list) or isinstance(tokens,tuple)):
+            raise YaccError("tokens must be a list or tuple.")
 
         # Check to see if a requires dictionary is defined.
         requires = ldict.get("require",None)
         if requires:
-            if not (isinstance(requires,types.DictType)):
-                raise YaccError,"require must be a dictionary."
+            if not (isinstance(requires,dict)):
+                raise YaccError("require must be a dictionary.")
 
-            for r,v in requires.items():
+            for r,v in list(requires.items()):
                 try:
-                    if not (isinstance(v,types.ListType)):
+                    if not (isinstance(v,list)):
                         raise TypeError
                     v1 = [x.split(".") for x in v]
                     Requires[r] = v1
-                except StandardError:
-                    print >>sys.stderr, "Invalid specification for rule '%s' in require. Expected a list of strings" % r            
+                except Exception:
+                    print("Invalid specification for rule '%s' in require. Expected a list of strings" % r, file=sys.stderr)            
 
         
         # Build the dictionary of terminals.  We a record a 0 in the
@@ -2063,12 +2064,12 @@ def yacc(method=default_lr, debug=yaccdebug, module=None, tabmodule=tab_module,
         # used in the grammar
 
         if 'error' in tokens:
-            print >>sys.stderr, "yacc: Illegal token 'error'.  Is a reserved word."
-            raise YaccError,"Illegal token name"
+            print("yacc: Illegal token 'error'.  Is a reserved word.", file=sys.stderr)
+            raise YaccError("Illegal token name")
 
         for n in tokens:
-            if Terminals.has_key(n):
-                print >>sys.stderr, "yacc: Warning. Token '%s' multiply defined." % n
+            if n in Terminals:
+                print("yacc: Warning. Token '%s' multiply defined." % n, file=sys.stderr)
             Terminals[n] = [ ]
 
         Terminals['error'] = [ ]
@@ -2076,13 +2077,13 @@ def yacc(method=default_lr, debug=yaccdebug, module=None, tabmodule=tab_module,
         # Get the precedence map (if any)
         prec = ldict.get("precedence",None)
         if prec:
-            if not (isinstance(prec,types.ListType) or isinstance(prec,types.TupleType)):
-                raise YaccError,"precedence must be a list or tuple."
+            if not (isinstance(prec,list) or isinstance(prec,tuple)):
+                raise YaccError("precedence must be a list or tuple.")
             add_precedence(prec)
-            Signature.update(repr(prec))
+            Signature.update(repr(prec).encode('utf-8'))
 
         for n in tokens:
-            if not Precedence.has_key(n):
+            if n not in Precedence:
                 Precedence[n] = ('right',0)         # Default, right associative, 0 precedence
 
         # Look for error handler
@@ -2093,76 +2094,84 @@ def yacc(method=default_lr, debug=yaccdebug, module=None, tabmodule=tab_module,
             elif isinstance(ef, types.MethodType):
                 ismethod = 1
             else:
-                raise YaccError,"'p_error' defined, but is not a function or method."                
-            eline = ef.func_code.co_firstlineno
-            efile = ef.func_code.co_filename
+                raise YaccError("'p_error' defined, but is not a function or method.")                
+            eline = ef.__code__.co_firstlineno
+            efile = ef.__code__.co_filename
             files[efile] = None
 
-            if (ef.func_code.co_argcount != 1+ismethod):
-                raise YaccError,"%s:%d: p_error() requires 1 argument." % (efile,eline)
+            if (ef.__code__.co_argcount != 1+ismethod):
+                raise YaccError("%s:%d: p_error() requires 1 argument." % (efile,eline))
             global Errorfunc
             Errorfunc = ef
         else:
-            print >>sys.stderr, "yacc: Warning. no p_error() function is defined."
+            print("yacc: Warning. no p_error() function is defined.", file=sys.stderr)
             
         # Get the list of built-in functions with p_ prefix
-        symbols = [ldict[f] for f in ldict.keys()
+        symbols = [ldict[f] for f in list(ldict.keys())
                if (type(ldict[f]) in (types.FunctionType, types.MethodType) and ldict[f].__name__[:2] == 'p_'
                    and ldict[f].__name__ != 'p_error')]
 
         # Check for non-empty symbols
         if len(symbols) == 0:
-            raise YaccError,"no rules of the form p_rulename are defined."
-    
+            raise YaccError("no rules of the form p_rulename are defined.")
+
+
+        '''
         # Sort the symbols by line number
-        symbols.sort(lambda x,y: cmp(x.func_code.co_firstlineno,y.func_code.co_firstlineno))
+        #### define cmp as this function has been removed in python3
+         '''
+        def cmp(a,b):
+           return (a > b) - (a < b)
+        symbols = sorted(symbols,key=functools.cmp_to_key(lambda x,y:cmp(x.__code__.co_firstlineno,y.__code__.co_firstlineno)))
+        ###symbols.sort(lambda x,y: cmp(x.__code__.co_firstlineno,y.__code__.co_firstlineno))
+
 
         # Add all of the symbols to the grammar
         for f in symbols:
             if (add_function(f)) < 0:
                 error += 1
             else:
-                files[f.func_code.co_filename] = None
+                files[f.__code__.co_filename] = None
 
         # Make a signature of the docstrings
         for f in symbols:
             if f.__doc__:
-                Signature.update(f.__doc__)
+                Signature.update(f.__doc__.encode('utf-8'))
     
         lr_init_vars()
 
         if error:
-            raise YaccError,"Unable to construct parser."
+            raise YaccError("Unable to construct parser.")
 
         if not lr_read_tables(tabmodule):
 
             # Validate files
-            for filename in files.keys():
+            for filename in list(files.keys()):
                 if not validate_file(filename):
                     error = 1
 
             # Validate dictionary
             validate_dict(ldict)
 
-            if start and not Prodnames.has_key(start):
-                raise YaccError,"Bad starting symbol '%s'" % start
+            if start and start not in Prodnames:
+                raise YaccError("Bad starting symbol '%s'" % start)
         
             augment_grammar(start)    
             error = verify_productions(cycle_check=check_recursion)
-            otherfunc = [ldict[f] for f in ldict.keys()
+            otherfunc = [ldict[f] for f in list(ldict.keys())
                if (type(f) in (types.FunctionType,types.MethodType) and ldict[f].__name__[:2] != 'p_')]
 
             if error:
-                raise YaccError,"Unable to construct parser."
+                raise YaccError("Unable to construct parser.")
             
             build_lritems()
             compute_first1()
             compute_follow(start)
         
-            if method in ['SLR','LALR']:
+            if method in [b'SLR',b'LALR']:
                 lr_parse_table(method)
             else:
-                raise YaccError, "Unknown parsing method '%s'" % method
+               raise YaccError("Unknown parsing method '%s'" % method)
 
             if write_tables:
                 lr_write_tables(tabmodule,outputdir)        
@@ -2174,8 +2183,8 @@ def yacc(method=default_lr, debug=yaccdebug, module=None, tabmodule=tab_module,
                     f.write("\n\n")
                     f.write(_vf.getvalue())
                     f.close()
-                except IOError,e:
-                    print >>sys.stderr, "yacc: can't create '%s'" % debugfile,e
+                except IOError as e:
+                    print("yacc: can't create '%s'" % debugfile,e, file=sys.stderr)
         
     # Made it here.   Create a parser object and set up its internal state.
     # Set global parse() method to bound method of parser object.
@@ -2220,5 +2229,5 @@ def yacc_cleanup():
     
 # Stub that raises an error if parsing is attempted without first calling yacc()
 def parse(*args,**kwargs):
-    raise YaccError, "yacc: No parser built with yacc()"
+    raise YaccError("yacc: No parser built with yacc()")
 
diff --git a/boolean2/ruleparser.py b/boolean2/ruleparser.py
index f49da7f..808697f 100644
--- a/boolean2/ruleparser.py
+++ b/boolean2/ruleparser.py
@@ -2,8 +2,8 @@
 Grammar file for a boolean parser based on PLY
 """
 import random, time, sys
-import tokenizer, util, state
-from ply import yacc
+from . import tokenizer, util, state
+from .ply import yacc
 from itertools import *
 
 # a list of all valid modes
@@ -166,7 +166,7 @@ class Parser(object):
         self.uninit_nodes = self.update_nodes - self.init_nodes
 
         # populate the initializer lines
-        self.init_lines = map( tokenizer.tok2line, self.init_tokens )
+        self.init_lines = list(map( tokenizer.tok2line, self.init_tokens ))
 
         # populate the body by the ranks            
         labelmap = {} 
@@ -191,8 +191,8 @@ class Parser(object):
         # by shuffling, sorting or reorganizing this body we can
         # implement various updating rule selection strategies
         self.update_lines = {}
-        for key, values in labelmap.items():
-            self.update_lines.setdefault(key, []).extend( map(tokenizer.tok2line, values))
+        for key, values in list(labelmap.items()):
+            self.update_lines.setdefault(key, []).extend( list(map(tokenizer.tok2line, values)))
 
 def test():
 
diff --git a/boolean2/state.py b/boolean2/state.py
index 7af27dd..0da52da 100644
--- a/boolean2/state.py
+++ b/boolean2/state.py
@@ -29,7 +29,7 @@ class State(object):
 
     def __repr__(self):  
         "Default string format"
-        items = [ '%s=%s' % x for x in self.items() ]
+        items = [ '%s=%s' % x for x in list(self.items()) ]
         items = ', '.join(items)
         return 'State: %s' % items
    
@@ -39,14 +39,14 @@ class State(object):
 
     def keys(self):
         "Returns the sorted keys"
-        return [ x for x,y in self.items() ]
+        return [ x for x,y in list(self.items()) ]
 
     def values(self):
         "Returns the values by sorted keys"
-        return [ y for x,y in self.items() ]
+        return [ y for x,y in list(self.items()) ]
 
     def __iter__(self):
-        return iter( self.keys() )
+        return iter( list(self.keys()) )
 
     def copy(self):            
         "Duplicates itself"
@@ -69,7 +69,7 @@ class State(object):
     
     def bin( self ):
         "A binary representation of the states"
-        values = map(str, map(int, self.values()))
+        values = list(map(str, list(map(int, list(self.values())))))
         return ''.join(values)
 
 def bit2int(bits):
@@ -123,10 +123,10 @@ def all_initial_states( nodes, limit=None ):
     def generator( nodes ):
         nodes = list(sorted(nodes))
         size  = len(nodes)
-        for index in xrange( 2 ** size ):
+        for index in range( 2 ** size ):
             bits  = int2bit(index, w=size )
-            bools = map(bool, bits)
-            store = dict( zip(nodes, bools) )
+            bools = list(map(bool, bits))
+            store = dict( list(zip(nodes, bools)) )
             def lookup( node ):
                 return store[node]
             yield store, lookup
@@ -147,5 +147,5 @@ if __name__ == '__main__':
     gen = all_initial_states(nodes)
 
     for data, func in gen:
-        print map(func, nodes)
+        print(list(map(func, nodes)))
 
diff --git a/boolean2/timemodel.py b/boolean2/timemodel.py
index 4955e7e..30eb628 100644
--- a/boolean2/timemodel.py
+++ b/boolean2/timemodel.py
@@ -1,6 +1,6 @@
-import util
-import ruleparser
-from boolmodel import BoolModel
+from . import util
+from . import ruleparser
+from .boolmodel import BoolModel
 
 class TimeModel( BoolModel ):
 
@@ -16,7 +16,7 @@ class TimeModel( BoolModel ):
         self.step  = 0
         self.times = [ 0 ]
 
-    def next(self):
+    def __next__(self):
         "Generates the updates based on the next simulation step"
         self.step += 1
         timestep = self.step * self.gcd             
@@ -33,7 +33,7 @@ class TimeModel( BoolModel ):
         while 1:
             
             # skip ahead until something valid
-            value = self.next()
+            value = next(self)
             tstep = value[0]
             rules = value[1:]
             if rules:
@@ -47,11 +47,11 @@ class TimeModel( BoolModel ):
         Iterates over the lines 'steps' times. 
         """
         shuffler = shuffler or self.shuffler
-        for index in xrange(steps):
+        for index in range(steps):
             self.parser.RULE_START_ITERATION( index, self )
             BoolModel.state_update(self)
             lines = shuffler( )
-            map( self.local_parse, lines ) 
+            list(map( self.local_parse, lines )) 
 
 if __name__ == '__main__':
     
@@ -78,8 +78,8 @@ if __name__ == '__main__':
     c = count(0)
     model.iterate( steps=12 )
     for state in model.states:
-        t = c.next() * 5
+        t = next(c) * 5
         tstamp = 'T=%d ' % t
-        data = [ tstamp ] + map(int, (state.A, state.B, state.C, state.D))
-        data = map(str, data)
-        print '\t'.join(data)
+        data = [ tstamp ] + list(map(int, (state.A, state.B, state.C, state.D)))
+        data = list(map(str, data))
+        print('\t'.join(data))
diff --git a/boolean2/tokenizer.py b/boolean2/tokenizer.py
index 7c46086..0a05e10 100644
--- a/boolean2/tokenizer.py
+++ b/boolean2/tokenizer.py
@@ -3,7 +3,8 @@ Main tokenizer.
 """
 from itertools import *
 import sys, random
-import util
+from . import util
+#from . import ply.lex as lex
 import ply.lex as lex
 
 class Lexer:
@@ -85,7 +86,7 @@ class Lexer:
     
     def tokenize_text(self, text):
         "Runs the lexer on text and returns a list of lists of tokens"
-        return map( self.tokenize_line, util.split(text) )
+        return list(map( self.tokenize_line, util.split(text) ))
 
 def init_tokens( tokenlist ):
     """
@@ -93,7 +94,7 @@ def init_tokens( tokenlist ):
     """
     def cond( elem ):
         return elem[1].type == 'EQUAL'
-    return filter( cond, tokenlist)
+    return list(filter( cond, tokenlist))
 
 def label_tokens( tokenlist ):
     """
@@ -102,7 +103,7 @@ def label_tokens( tokenlist ):
     """
     def cond( elem ):
         return elem[0].type == 'LABEL'
-    return filter( cond, tokenlist)
+    return list(filter( cond, tokenlist))
 
 def async_tokens( tokenlist ):
     """
@@ -111,7 +112,7 @@ def async_tokens( tokenlist ):
     """
     def cond( elem ):
         return elem[1].type == 'ASSIGN'
-    return filter( cond, tokenlist)
+    return list(filter( cond, tokenlist))
 
 def update_tokens( tokenlist ):
     """
@@ -119,7 +120,7 @@ def update_tokens( tokenlist ):
     """
     def cond( elem ):
         return elem[1].type == 'ASSIGN' or elem[2].type == 'ASSIGN'
-    return filter( cond, tokenlist)
+    return list(filter( cond, tokenlist))
 
 def get_nodes( tokenlist ):
     """
@@ -132,7 +133,7 @@ def get_nodes( tokenlist ):
     def get( token):
         return token.value
 
-    nodes = map(get, filter( cond, chain( *tokenlist )))
+    nodes = list(map(get, list(filter( cond, chain( *tokenlist )))))
     nodes = set(nodes)
     util.check_case( nodes )
     return nodes
@@ -197,7 +198,7 @@ def modify_states( text, turnon=[], turnoff=[] ):
     tokens  = tokenize( text )
 
     init = init_tokens( tokens )
-    init_lines = map(tok2line, init)
+    init_lines = list(map(tok2line, init))
 
     # override the initial values
     init_lines.extend( [ '%s=True'  % node for node in turnon  ] )
@@ -230,6 +231,6 @@ if __name__ == '__main__':
 
     """
     
-    print modify_states( text, turnon=['A', 'B'], turnoff=['C'] )
+    print(modify_states( text, turnon=['A', 'B'], turnoff=['C'] ))
 
-    
\ No newline at end of file
+    
diff --git a/boolean2/util.py b/boolean2/util.py
index 850169c..a03ea02 100644
--- a/boolean2/util.py
+++ b/boolean2/util.py
@@ -1,4 +1,5 @@
 import sys, random, pickle
+from functools import reduce
 
 #
 # handy shortcuts
@@ -17,12 +18,12 @@ def join( data, sep="\t", patt="%s\n"):
 
 def error(msg):
     "Prints an error message and stops"
-    print '*** error: %s' % msg
+    print('*** error: %s' % msg)
     sys.exit()
 
 def warn(msg):
     "Prints a warning message"
-    print '*** warning: %s' % msg
+    print('*** warning: %s' % msg)
  
 def tuple_to_bool( value ):
     """
@@ -50,7 +51,7 @@ def split( text ):
     """
     Strips lines and returns nonempty lines
     """
-    return filter(notcomment, map(strip, text.splitlines()))
+    return list(filter(notcomment, list(map(strip, text.splitlines()))))
 
 def default_shuffler( lines ):
     "Default shuffler"
@@ -74,8 +75,8 @@ def detect_cycles( data ):
     fsize   = len(data)
 
     # maximum size
-    for msize in xrange(1, fsize/2+1):
-        for index in xrange(fsize):
+    for msize in range(1, int(fsize/2+1)):
+        for index in range(fsize):
             left  = data[index:index+msize]
             right = data[index+msize:index+2*msize]
             if left == right:
@@ -152,7 +153,7 @@ class Collector(object):
             if normalize:
                 def divide(x):
                     return x/size
-                values = map(divide, values)
+                values = list(map(divide, values))
             out[node] = values
         return out
 
@@ -161,4 +162,4 @@ def test():
     doctest.testmod()
 
 if __name__ == '__main__':
-    test()
\ No newline at end of file
+    test()
diff --git a/tests/test_all.py b/tests/test_all.py
index 2e8f75f..b03593b 100755
--- a/tests/test_all.py
+++ b/tests/test_all.py
@@ -2,7 +2,8 @@ import unittest
 import testbase
 
 # these are the module names that will be tested
-modules = "test_sync test_time"
+#modules = "test_sync test_time"
+modules = "test_sync"
 
 def get_suite():
     suite = unittest.TestSuite()
diff --git a/tests/test_engine.py b/tests/test_engine.py
index ad712f9..7f05e14 100755
--- a/tests/test_engine.py
+++ b/tests/test_engine.py
@@ -115,10 +115,10 @@ class EngineTest( unittest.TestCase ):
             funcs  = [ partial( get, attr=attr ) for attr in 'ABC' ]
 
             # map to the data
-            values = [ map( f, states ) for f in funcs ]
+            values = [ list(map( f, states )) for f in funcs ]
             
             # filter for true value
-            trues  = [ filter ( istrue, v) for v in values ]
+            trues  = [ list(filter ( istrue, v)) for v in values ]
 
             # true values 
             A, B, C = trues
@@ -208,9 +208,9 @@ class EngineTest( unittest.TestCase ):
         # execute the code in python for a number of steps
         # having too many steps is bad as it falls into a steady state
         steps = 4
-        exec init_text
+        exec(init_text)
         for i in range( steps ):
-            exec py_text in locals()
+            exec(py_text, locals())
         
         # see the full text here
         #print full_text
diff --git a/tests/test_engine_noranks.py b/tests/test_engine_noranks.py
index 1f7e391..5c7da01 100644
--- a/tests/test_engine_noranks.py
+++ b/tests/test_engine_noranks.py
@@ -115,10 +115,10 @@ class ModelTest( unittest.TestCase ):
             funcs  = [ partial( get, attr=attr ) for attr in 'ABC' ]
 
             # map to the data
-            values = [ map( f, states ) for f in funcs ]
+            values = [ list(map( f, states )) for f in funcs ]
             
             # filter for true value
-            trues  = [ filter ( istrue, v) for v in values ]
+            trues  = [ list(filter ( istrue, v)) for v in values ]
 
             # true values 
             A, B, C = trues
@@ -208,9 +208,9 @@ class ModelTest( unittest.TestCase ):
         # execute the code in python for a number of steps
         # having too many steps is bad as it falls into a steady state
         steps = 4
-        exec init_text
+        exec(init_text)
         for i in range( steps ):
-            exec py_text in locals()
+            exec(py_text, locals())
         
         # see the full text here
         #print full_text
diff --git a/tests/test_sync.py b/tests/test_sync.py
index 7cae278..58e25be 100644
--- a/tests/test_sync.py
+++ b/tests/test_sync.py
@@ -81,7 +81,8 @@ class SyncTest( testbase.TestBase ):
         """
 
         # valid nodes
-        nodes  = string.uppercase[:]
+        #nodes  = string.uppercase[:]
+        nodes  = string.ascii_uppercase
         join = testbase.join
 
         #
@@ -113,7 +114,7 @@ class SyncTest( testbase.TestBase ):
             # insert some parentheses
             if randint(1, 100) > 30:
                 for i in range(2):
-                    half = size/2
+                    half = int(size/2) #size/2
                     left, right = randint(0, half), randint(half, size)
                     targets[left]  = '(' + targets[left]
                     targets[right] = targets[right] + ')'
@@ -152,9 +153,9 @@ class SyncTest( testbase.TestBase ):
         # execute the code in python for a number of steps
         # having too many steps is actually counterproductive as it falls into a steady state
         steps = 4
-        exec init_text
+        exec(init_text)
         for i in range( steps ):
-            exec py_text in locals()
+            exec(py_text, locals())
         
         # see the full text here
         #print full_text
